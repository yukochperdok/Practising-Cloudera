1. Nos ayudaremos de la mysql que instala cloudera por defecto

[cloudera@quickstart ~]$ mysql -u root -pcloudera
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 118
Server version: 5.1.73 Source distribution

Copyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| cm                 |
| firehose           |
| hue                |
| metastore          |
| movielens          |
| mysql              |
| nav                |
| navms              |
| oozie              |
| retail_db          |
| rman               |
| sentry             |
+--------------------+
13 rows in set (0.00 sec)

mysql> use retail_db;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables;
+---------------------+
| Tables_in_retail_db |
+---------------------+
| categories          |
| customers           |
| departments         |
| order_items         |
| orders              |
| products            |
+---------------------+
6 rows in set (0.00 sec)

mysql> select * from orders limit 10;
+----------+---------------------+-------------------+-----------------+
| order_id | order_date          | order_customer_id | order_status    |
+----------+---------------------+-------------------+-----------------+
|        1 | 2013-07-25 00:00:00 |             11599 | CLOSED          |
|        2 | 2013-07-25 00:00:00 |               256 | PENDING_PAYMENT |
|        3 | 2013-07-25 00:00:00 |             12111 | COMPLETE        |
|        4 | 2013-07-25 00:00:00 |              8827 | CLOSED          |
|        5 | 2013-07-25 00:00:00 |             11318 | COMPLETE        |
|        6 | 2013-07-25 00:00:00 |              7130 | COMPLETE        |
|        7 | 2013-07-25 00:00:00 |              4530 | COMPLETE        |
|        8 | 2013-07-25 00:00:00 |              2911 | PROCESSING      |
|        9 | 2013-07-25 00:00:00 |              5657 | PENDING_PAYMENT |
|       10 | 2013-07-25 00:00:00 |              5648 | PENDING_PAYMENT |
+----------+---------------------+-------------------+-----------------+
10 rows in set (0.00 sec)

mysql> select * from categories limit 10;
+-------------+------------------------+---------------------+
| category_id | category_department_id | category_name       |
+-------------+------------------------+---------------------+
|           1 |                      2 | Football            |
|           2 |                      2 | Soccer              |
|           3 |                      2 | Baseball & Softball |
|           4 |                      2 | Basketball          |
|           5 |                      2 | Lacrosse            |
|           6 |                      2 | Tennis & Racquet    |
|           7 |                      2 | Hockey              |
|           8 |                      2 | More Sports         |
|           9 |                      3 | Cardio Equipment    |
|          10 |                      3 | Strength Training   |
+-------------+------------------------+---------------------+
10 rows in set (0.00 sec)

mysql> select * from customers limit 10;
+-------------+----------------+----------------+----------------+-------------------+-----------------------------+---------------+----------------+------------------+
| customer_id | customer_fname | customer_lname | customer_email | customer_password | customer_street             | customer_city | customer_state | customer_zipcode |
+-------------+----------------+----------------+----------------+-------------------+-----------------------------+---------------+----------------+------------------+
|           1 | Richard        | Hernandez      | XXXXXXXXX      | XXXXXXXXX         | 6303 Heather Plaza          | Brownsville   | TX             | 78521            |
|           2 | Mary           | Barrett        | XXXXXXXXX      | XXXXXXXXX         | 9526 Noble Embers Ridge     | Littleton     | CO             | 80126            |
|           3 | Ann            | Smith          | XXXXXXXXX      | XXXXXXXXX         | 3422 Blue Pioneer Bend      | Caguas        | PR             | 00725            |
|           4 | Mary           | Jones          | XXXXXXXXX      | XXXXXXXXX         | 8324 Little Common          | San Marcos    | CA             | 92069            |
|           5 | Robert         | Hudson         | XXXXXXXXX      | XXXXXXXXX         | 10 Crystal River Mall       | Caguas        | PR             | 00725            |
|           6 | Mary           | Smith          | XXXXXXXXX      | XXXXXXXXX         | 3151 Sleepy Quail Promenade | Passaic       | NJ             | 07055            |
|           7 | Melissa        | Wilcox         | XXXXXXXXX      | XXXXXXXXX         | 9453 High Concession        | Caguas        | PR             | 00725            |
|           8 | Megan          | Smith          | XXXXXXXXX      | XXXXXXXXX         | 3047 Foggy Forest Plaza     | Lawrence      | MA             | 01841            |
|           9 | Mary           | Perez          | XXXXXXXXX      | XXXXXXXXX         | 3616 Quaking Street         | Caguas        | PR             | 00725            |
|          10 | Melissa        | Smith          | XXXXXXXXX      | XXXXXXXXX         | 8598 Harvest Beacon Plaza   | Stafford      | VA             | 22554            |
+-------------+----------------+----------------+----------------+-------------------+-----------------------------+---------------+----------------+------------------+
10 rows in set (0.00 sec)

mysql> exit
Bye
[cloudera@quickstart ~]$ 



2. Vamos lanzando consultas para cargar estas 3 tablas en HDFS a traves de SQOOP. O bien las acumulamos todas en un .sh y lo lanzamos por consola.
Las 3 consultas son:
a) sqoop import --table categories --connect jdbc:mysql://localhost/retail_db --username root --password cloudera

--> Me conecto a la BBDD (por cadena conexion: jdbc:mysql://localhost/retail_db --username root --password cloudera) y me descargo la tabla categories. 
Como no indico directorio ni tipo de separador se me descargara en /user/cloudera/categories un fichero con todos los registros cuyo separador será ','.




b) sqoop import --table customers --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --warehouse-dir /tmp/sqoop/ --columns "customer_fname,customer_lname,customer_street"

--> Me conecto a la BBDD (por cadena conexion: jdbc:mysql://localhost/retail_db --username root --password cloudera) y me descargo la tabla customers. Y en concreto
las columnas: customer_fname,customer_lname,customer_street. 
Como no indico tipo de separador se me descargara un fichero con todos los registros cuyo separador será ','.
Si estoy indicando el directorio de destino en HDFS, por lo tanto se me descargara en /tmp/sqoop/ un directorio customers con el fichero.


c) sqoop import --table orders --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --incremental append --check-column order_id --last-value 5000

-->Me conecto a la BBDD (por cadena conexion: jdbc:mysql://localhost/retail_db --username root --password cloudera) y me descargo la tabla orders. De forma incremental, utilizando como columna de checkeo el order_id, para todos los valores superiores al 5000.
Como no indico directorio ni tipo de separador se me descargara en /user/cloudera/orders un fichero con todos los registros cuyo separador será ','.



3. El script quedaria de la siguiente forma:
#!/bin/bash

sqoop import --table categories --connect jdbc:mysql://localhost/retail_db --username root --password cloudera
sqoop import --table customers --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --warehouse-dir /tmp/sqoop/ --columns "customer_fname,customer_lname,customer_street"
sqoop import --table orders --connect jdbc:mysql://localhost/retail_db --username root --password cloudera --incremental append --check-column order_id --last-value 5000

4. Vemos lo que existe actualmente:

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera
Found 7 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-22 12:09 /user/cloudera/.Trash
drwxr-xr-x   - cloudera cloudera          0 2016-06-25 02:09 /user/cloudera/.sparkStaging
drwxr-xr-x   - cloudera cloudera          0 2016-05-14 03:43 /user/cloudera/departments
drwxr-xr-x   - cloudera cloudera          0 2016-05-30 10:55 /user/cloudera/ejerc_evaluable
drwxr-xr-x   - cloudera cloudera          0 2016-05-18 10:16 /user/cloudera/ejercicios
drwxr-xr-x   - cloudera cloudera          0 2016-07-22 16:00 /user/cloudera/pruebasComandos
drwxr-xr-x   - cloudera cloudera          0 2016-07-23 12:15 /user/cloudera/training
[cloudera@quickstart ~]$ hadoop fs -ls /tmp/sqoop
[cloudera@quickstart ~]$ 


5. Lanzamos el script

[cloudera@quickstart ~]$ cd /home/cloudera/Desktop/Curso/Ejercicios_Fernando/SQOOP
[cloudera@quickstart SQOOP]$ ls -ltr
total 28
-rwxrwxrwx 1 cloudera cloudera  490 May 20 08:39 script_sqoop.sh

[cloudera@quickstart SQOOP]$ ./script_sqoop.sh
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
16/07/27 10:53:49 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
16/07/27 10:53:49 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/07/27 10:53:49 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/07/27 10:53:49 INFO tool.CodeGenTool: Beginning code generation
16/07/27 10:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `categories` AS t LIMIT 1
16/07/27 10:53:49 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `categories` AS t LIMIT 1
16/07/27 10:53:49 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/07442162c47bc08c329351e2390b6c54/categories.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/07/27 10:53:51 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/07442162c47bc08c329351e2390b6c54/categories.jar
16/07/27 10:53:51 WARN manager.MySQLManager: It looks like you are importing from mysql.
16/07/27 10:53:51 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
16/07/27 10:53:51 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
16/07/27 10:53:51 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
16/07/27 10:53:51 INFO mapreduce.ImportJobBase: Beginning import of categories
16/07/27 10:53:51 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
16/07/27 10:53:51 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/07/27 10:53:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/07/27 10:53:52 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/07/27 10:53:54 INFO db.DBInputFormat: Using read commited transaction isolation
16/07/27 10:53:54 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`category_id`), MAX(`category_id`) FROM `categories`
16/07/27 10:53:54 INFO db.IntegerSplitter: Split size: 14; Num splits: 4 from: 1 to: 58
16/07/27 10:53:54 INFO mapreduce.JobSubmitter: number of splits:4
16/07/27 10:53:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469299672451_0007
16/07/27 10:53:55 INFO impl.YarnClientImpl: Submitted application application_1469299672451_0007
16/07/27 10:53:55 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1469299672451_0007/
16/07/27 10:53:55 INFO mapreduce.Job: Running job: job_1469299672451_0007
16/07/27 10:54:01 INFO mapreduce.Job: Job job_1469299672451_0007 running in uber mode : false
16/07/27 10:54:01 INFO mapreduce.Job:  map 0% reduce 0%
16/07/27 10:54:08 INFO mapreduce.Job:  map 25% reduce 0%
16/07/27 10:54:09 INFO mapreduce.Job:  map 50% reduce 0%
16/07/27 10:54:10 INFO mapreduce.Job:  map 75% reduce 0%
16/07/27 10:54:11 INFO mapreduce.Job:  map 100% reduce 0%
16/07/27 10:54:11 INFO mapreduce.Job: Job job_1469299672451_0007 completed successfully
16/07/27 10:54:11 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=554492
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=472
		HDFS: Number of bytes written=1029
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=4
		Other local map tasks=4
		Total time spent by all maps in occupied slots (ms)=19436
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=19436
		Total vcore-seconds taken by all map tasks=19436
		Total megabyte-seconds taken by all map tasks=19902464
	Map-Reduce Framework
		Map input records=58
		Map output records=58
		Input split bytes=472
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=281
		CPU time spent (ms)=3780
		Physical memory (bytes) snapshot=851021824
		Virtual memory (bytes) snapshot=6268203008
		Total committed heap usage (bytes)=1117782016
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=1029
16/07/27 10:54:11 INFO mapreduce.ImportJobBase: Transferred 1.0049 KB in 19.2244 seconds (53.5258 bytes/sec)
16/07/27 10:54:11 INFO mapreduce.ImportJobBase: Retrieved 58 records.
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
16/07/27 10:54:13 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
16/07/27 10:54:13 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/07/27 10:54:13 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/07/27 10:54:13 INFO tool.CodeGenTool: Beginning code generation
16/07/27 10:54:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `customers` AS t LIMIT 1
16/07/27 10:54:13 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `customers` AS t LIMIT 1
16/07/27 10:54:13 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/321ecd50a95e8d755ee493de21e21fd7/customers.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/07/27 10:54:15 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/321ecd50a95e8d755ee493de21e21fd7/customers.jar
16/07/27 10:54:15 WARN manager.MySQLManager: It looks like you are importing from mysql.
16/07/27 10:54:15 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
16/07/27 10:54:15 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
16/07/27 10:54:15 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
16/07/27 10:54:15 INFO mapreduce.ImportJobBase: Beginning import of customers
16/07/27 10:54:15 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
16/07/27 10:54:15 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/07/27 10:54:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/07/27 10:54:16 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/07/27 10:54:18 INFO db.DBInputFormat: Using read commited transaction isolation
16/07/27 10:54:18 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`customer_id`), MAX(`customer_id`) FROM `customers`
16/07/27 10:54:18 INFO db.IntegerSplitter: Split size: 3108; Num splits: 4 from: 1 to: 12435
16/07/27 10:54:18 INFO mapreduce.JobSubmitter: number of splits:4
16/07/27 10:54:18 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469299672451_0008
16/07/27 10:54:18 INFO impl.YarnClientImpl: Submitted application application_1469299672451_0008
16/07/27 10:54:18 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1469299672451_0008/
16/07/27 10:54:18 INFO mapreduce.Job: Running job: job_1469299672451_0008
16/07/27 10:54:24 INFO mapreduce.Job: Job job_1469299672451_0008 running in uber mode : false
16/07/27 10:54:24 INFO mapreduce.Job:  map 0% reduce 0%
16/07/27 10:54:32 INFO mapreduce.Job:  map 50% reduce 0%
16/07/27 10:54:33 INFO mapreduce.Job:  map 75% reduce 0%
16/07/27 10:54:34 INFO mapreduce.Job:  map 100% reduce 0%
16/07/27 10:54:34 INFO mapreduce.Job: Job job_1469299672451_0008 completed successfully
16/07/27 10:54:35 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=555740
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=487
		HDFS: Number of bytes written=421165
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=4
		Other local map tasks=4
		Total time spent by all maps in occupied slots (ms)=20173
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=20173
		Total vcore-seconds taken by all map tasks=20173
		Total megabyte-seconds taken by all map tasks=20657152
	Map-Reduce Framework
		Map input records=12435
		Map output records=12435
		Input split bytes=487
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=227
		CPU time spent (ms)=4680
		Physical memory (bytes) snapshot=850296832
		Virtual memory (bytes) snapshot=6288457728
		Total committed heap usage (bytes)=952107008
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=421165
16/07/27 10:54:35 INFO mapreduce.ImportJobBase: Transferred 411.2939 KB in 18.7801 seconds (21.9006 KB/sec)
16/07/27 10:54:35 INFO mapreduce.ImportJobBase: Retrieved 12435 records.
Warning: /usr/lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
16/07/27 10:54:36 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.7.0
16/07/27 10:54:36 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
16/07/27 10:54:37 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
16/07/27 10:54:37 INFO tool.CodeGenTool: Beginning code generation
16/07/27 10:54:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `orders` AS t LIMIT 1
16/07/27 10:54:37 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `orders` AS t LIMIT 1
16/07/27 10:54:37 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce
Note: /tmp/sqoop-cloudera/compile/c6787def586b1ac7fde2c233adad3678/orders.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
16/07/27 10:54:38 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-cloudera/compile/c6787def586b1ac7fde2c233adad3678/orders.jar
16/07/27 10:54:39 INFO tool.ImportTool: Maximal id query for free form incremental import: SELECT MAX(`order_id`) FROM `orders`
16/07/27 10:54:39 INFO tool.ImportTool: Incremental import based on column `order_id`
16/07/27 10:54:39 INFO tool.ImportTool: Lower bound value: 5000
16/07/27 10:54:39 INFO tool.ImportTool: Upper bound value: 68883
16/07/27 10:54:39 WARN manager.MySQLManager: It looks like you are importing from mysql.
16/07/27 10:54:39 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
16/07/27 10:54:39 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
16/07/27 10:54:39 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
16/07/27 10:54:39 INFO mapreduce.ImportJobBase: Beginning import of orders
16/07/27 10:54:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address
16/07/27 10:54:39 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
16/07/27 10:54:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
16/07/27 10:54:39 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
16/07/27 10:54:41 INFO db.DBInputFormat: Using read commited transaction isolation
16/07/27 10:54:41 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`order_id`), MAX(`order_id`) FROM `orders` WHERE ( `order_id` > 5000 AND `order_id` <= 68883 )
16/07/27 10:54:41 INFO db.IntegerSplitter: Split size: 15970; Num splits: 4 from: 5001 to: 68883
16/07/27 10:54:41 INFO mapreduce.JobSubmitter: number of splits:4
16/07/27 10:54:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1469299672451_0009
16/07/27 10:54:41 INFO impl.YarnClientImpl: Submitted application application_1469299672451_0009
16/07/27 10:54:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1469299672451_0009/
16/07/27 10:54:41 INFO mapreduce.Job: Running job: job_1469299672451_0009
16/07/27 10:54:49 INFO mapreduce.Job: Job job_1469299672451_0009 running in uber mode : false
16/07/27 10:54:49 INFO mapreduce.Job:  map 0% reduce 0%
16/07/27 10:54:57 INFO mapreduce.Job:  map 25% reduce 0%
16/07/27 10:54:58 INFO mapreduce.Job:  map 50% reduce 0%
16/07/27 10:55:00 INFO mapreduce.Job:  map 100% reduce 0%
16/07/27 10:55:00 INFO mapreduce.Job: Job job_1469299672451_0009 completed successfully
16/07/27 10:55:00 INFO mapreduce.Job: Counters: 30
	File System Counters
		FILE: Number of bytes read=0
		FILE: Number of bytes written=557188
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=472
		HDFS: Number of bytes written=2787671
		HDFS: Number of read operations=16
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Job Counters 
		Launched map tasks=4
		Other local map tasks=4
		Total time spent by all maps in occupied slots (ms)=27225
		Total time spent by all reduces in occupied slots (ms)=0
		Total time spent by all map tasks (ms)=27225
		Total vcore-seconds taken by all map tasks=27225
		Total megabyte-seconds taken by all map tasks=27878400
	Map-Reduce Framework
		Map input records=63883
		Map output records=63883
		Input split bytes=472
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=328
		CPU time spent (ms)=11060
		Physical memory (bytes) snapshot=830218240
		Virtual memory (bytes) snapshot=6292328448
		Total committed heap usage (bytes)=952107008
	File Input Format Counters 
		Bytes Read=0
	File Output Format Counters 
		Bytes Written=2787671
16/07/27 10:55:00 INFO mapreduce.ImportJobBase: Transferred 2.6585 MB in 20.3556 seconds (133.7386 KB/sec)
16/07/27 10:55:00 INFO mapreduce.ImportJobBase: Retrieved 63883 records.
16/07/27 10:55:00 INFO util.AppendUtils: Creating missing output directory - orders
16/07/27 10:55:00 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:
16/07/27 10:55:00 INFO tool.ImportTool:  --incremental append
16/07/27 10:55:00 INFO tool.ImportTool:   --check-column order_id
16/07/27 10:55:00 INFO tool.ImportTool:   --last-value 68883
16/07/27 10:55:00 INFO tool.ImportTool: (Consider saving this with 'sqoop job --create')
[cloudera@quickstart SQOOP]$ 



OBS: Solo genera ficheros map. No hay reduce. Por eso el reduce se queda al 0% y los ficheros generados seran part-m-XXXX.
OBS2: En la incremental lo que ha hecho es mirar el max_id_order:
SELECT MAX(`order_id`) FROM `orders`
Y luego ha recogido los que hay entre 5000 y ese maximo:
SELECT MIN(`order_id`), MAX(`order_id`) FROM `orders` WHERE ( `order_id` > 5000 AND `order_id` <= 68883 )

Al final te avisa que si vuelves a hecer otra incremental tendras que sustituir los argumentos:
16/07/27 10:55:00 INFO tool.ImportTool: Incremental import complete! To run another incremental import of all data following this import, supply the following arguments:
16/07/27 10:55:00 INFO tool.ImportTool:  --incremental append
16/07/27 10:55:00 INFO tool.ImportTool:   --check-column order_id
16/07/27 10:55:00 INFO tool.ImportTool:   --last-value 68883




6. Revisamos lo que hemos generado:

a) CATEGORIES

[cloudera@quickstart SQOOP]$ hadoop fs -ls /tmp/sqoop
Found 1 items
drwxr-xr-x   - cloudera supergroup          0 2016-07-27 10:54 /tmp/sqoop/customers
[cloudera@quickstart SQOOP]$ hadoop fs -ls /tmp/sqoop/customers
Found 5 items
-rw-r--r--   1 cloudera supergroup          0 2016-07-27 10:54 /tmp/sqoop/customers/_SUCCESS
-rw-r--r--   1 cloudera supergroup     105464 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00000
-rw-r--r--   1 cloudera supergroup     105148 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00001
-rw-r--r--   1 cloudera supergroup     105345 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00002
-rw-r--r--   1 cloudera supergroup     105208 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00003
[cloudera@quickstart SQOOP]$ hadoop fs -tail /tmp/sqoop/customers/part-m-00000
tre
Mary,Smith,8600 Red Goose Abbey
Bruce,Thornton,1763 Dusty Trace
Mary,Smith,7613 Amber Willow Manor
Timothy,Lee,7868 Iron Fawn Mall 
Mary,Fisher,5182 Noble Prairie Heath
Richard,Smith,4208 High Promenade
Barbara,Harris,2513 Sleepy Log Grounds
Mary,Hendricks,2002 Grand Hickory Line
Amy,Smith,3261 Green Circle
Sean,Bolton,8629 Heather Spring Wharf
Mary,Smith,5470 Indian Fawn Common
Carolyn,Rodriguez,9885 Hazy Forest Path
Mary,Yoder,1913 Tawny Park
Brian,Smith,1840 Fallen Lane
Mary,Jenkins,119 Round Vale
Patricia,Hayes,5755 Thunder Nectar Gate
Stephanie,Larson,3050 Cinder End
Douglas,Hanna,1112 Rustic Range
Mary,Smith,8217 Fallen Panda Walk
Brittany,Copeland,5735 Round Beacon Terrace
Mary,Smith,5436 Grand Hickory Farm
George,Reyes,8702 Silver Apple Square
Ralph,Dixon,5633 Harvest Turnabout
Mary,Wilkins,1213 Cotton Pike
Megan,Smith,5292 Shady Pony Cape
Mary,Stone,8510 Green River Acres
Samantha,Smith,355 Cozy Square
Tiffany,Estes,5182 Cotton Heath
Mary,Smith,577 Rustic Nectar Row
Jack,James,5876 Burning Mall 
[cloudera@quickstart SQOOP]$ 




b) CUSTOMERS

[cloudera@quickstart SQOOP]$ hadoop fs -ls /tmp/sqoop
Found 1 items
drwxr-xr-x   - cloudera supergroup          0 2016-07-27 10:54 /tmp/sqoop/customers
[cloudera@quickstart SQOOP]$ hadoop fs -ls /tmp/sqoop/customers
Found 5 items
-rw-r--r--   1 cloudera supergroup          0 2016-07-27 10:54 /tmp/sqoop/customers/_SUCCESS
-rw-r--r--   1 cloudera supergroup     105464 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00000
-rw-r--r--   1 cloudera supergroup     105148 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00001
-rw-r--r--   1 cloudera supergroup     105345 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00002
-rw-r--r--   1 cloudera supergroup     105208 2016-07-27 10:54 /tmp/sqoop/customers/part-m-00003
[cloudera@quickstart SQOOP]$ hadoop fs -tail /tmp/sqoop/customers/part-m-00000
tre
Mary,Smith,8600 Red Goose Abbey
Bruce,Thornton,1763 Dusty Trace
Mary,Smith,7613 Amber Willow Manor
Timothy,Lee,7868 Iron Fawn Mall 
Mary,Fisher,5182 Noble Prairie Heath
Richard,Smith,4208 High Promenade
Barbara,Harris,2513 Sleepy Log Grounds
Mary,Hendricks,2002 Grand Hickory Line
Amy,Smith,3261 Green Circle
Sean,Bolton,8629 Heather Spring Wharf
Mary,Smith,5470 Indian Fawn Common
Carolyn,Rodriguez,9885 Hazy Forest Path
Mary,Yoder,1913 Tawny Park
Brian,Smith,1840 Fallen Lane
Mary,Jenkins,119 Round Vale
Patricia,Hayes,5755 Thunder Nectar Gate
Stephanie,Larson,3050 Cinder End
Douglas,Hanna,1112 Rustic Range
Mary,Smith,8217 Fallen Panda Walk
Brittany,Copeland,5735 Round Beacon Terrace
Mary,Smith,5436 Grand Hickory Farm
George,Reyes,8702 Silver Apple Square
Ralph,Dixon,5633 Harvest Turnabout
Mary,Wilkins,1213 Cotton Pike
Megan,Smith,5292 Shady Pony Cape
Mary,Stone,8510 Green River Acres
Samantha,Smith,355 Cozy Square
Tiffany,Estes,5182 Cotton Heath
Mary,Smith,577 Rustic Nectar Row
Jack,James,5876 Burning Mall 
[cloudera@quickstart SQOOP]$ 




c) ORDERS

Observar que no existe el fichero _SUCCESS, seguramente porque es incremental. Y si vuelves a lanzarla te daria fallo.

[cloudera@quickstart SQOOP]$ hadoop fs -ls /user/cloudera
Found 10 items
drwxr-xr-x   - cloudera cloudera          0 2016-07-22 12:09 /user/cloudera/.Trash
drwxr-xr-x   - cloudera cloudera          0 2016-06-25 02:09 /user/cloudera/.sparkStaging
drwxr-xr-x   - cloudera cloudera          0 2016-07-27 10:55 /user/cloudera/_sqoop
drwxr-xr-x   - cloudera cloudera          0 2016-07-27 10:54 /user/cloudera/categories
drwxr-xr-x   - cloudera cloudera          0 2016-05-14 03:43 /user/cloudera/departments
drwxr-xr-x   - cloudera cloudera          0 2016-05-30 10:55 /user/cloudera/ejerc_evaluable
drwxr-xr-x   - cloudera cloudera          0 2016-05-18 10:16 /user/cloudera/ejercicios
drwxr-xr-x   - cloudera cloudera          0 2016-07-27 10:55 /user/cloudera/orders
drwxr-xr-x   - cloudera cloudera          0 2016-07-22 16:00 /user/cloudera/pruebasComandos
drwxr-xr-x   - cloudera cloudera          0 2016-07-23 12:15 /user/cloudera/training
[cloudera@quickstart SQOOP]$ hadoop fs -ls /user/cloudera/orders
Found 4 items
-rw-r--r--   1 cloudera cloudera     693244 2016-07-27 10:54 /user/cloudera/orders/part-m-00000
-rw-r--r--   1 cloudera cloudera     698164 2016-07-27 10:54 /user/cloudera/orders/part-m-00001
-rw-r--r--   1 cloudera cloudera     697863 2016-07-27 10:54 /user/cloudera/orders/part-m-00002
-rw-r--r--   1 cloudera cloudera     698400 2016-07-27 10:54 /user/cloudera/orders/part-m-00003
[cloudera@quickstart SQOOP]$ hadoop fs -tail /user/cloudera/orders/part-m-00000
013-12-01 00:00:00.0,8874,SUSPECTED_FRAUD
20950,2013-12-01 00:00:00.0,9734,CANCELED
20951,2013-12-01 00:00:00.0,9937,PENDING_PAYMENT
20952,2013-12-01 00:00:00.0,10541,PENDING_PAYMENT
20953,2013-12-01 00:00:00.0,8554,COMPLETE
20954,2013-12-01 00:00:00.0,7854,CLOSED
20955,2013-12-01 00:00:00.0,5243,PROCESSING
20956,2013-12-01 00:00:00.0,12291,COMPLETE
20957,2013-12-01 00:00:00.0,9407,COMPLETE
20958,2013-12-01 00:00:00.0,2462,CANCELED
20959,2013-12-01 00:00:00.0,2252,PAYMENT_REVIEW
20960,2013-12-01 00:00:00.0,7304,COMPLETE
20961,2013-12-01 00:00:00.0,8885,PENDING_PAYMENT
20962,2013-12-01 00:00:00.0,8489,COMPLETE
20963,2013-12-01 00:00:00.0,2207,PENDING_PAYMENT
20964,2013-12-01 00:00:00.0,821,ON_HOLD
20965,2013-12-01 00:00:00.0,7632,COMPLETE
20966,2013-12-01 00:00:00.0,10952,PROCESSING
20967,2013-12-01 00:00:00.0,2735,PAYMENT_REVIEW
20968,2013-12-01 00:00:00.0,1002,SUSPECTED_FRAUD
20969,2013-12-01 00:00:00.0,1880,PROCESSING
20970,2013-12-01 00:00:00.0,7371,PENDING
20971,2013-12-01 00:00:00.0,7929,PENDING_PAYMENT
[cloudera@quickstart SQOOP]$ hadoop fs -tail /user/cloudera/orders/part-m-00001
 00:00:00.0,892,COMPLETE
36921,2014-03-10 00:00:00.0,11147,CLOSED
36922,2014-03-10 00:00:00.0,9773,PENDING_PAYMENT
36923,2014-03-10 00:00:00.0,11460,COMPLETE
36924,2014-03-10 00:00:00.0,8873,PENDING_PAYMENT
36925,2014-03-10 00:00:00.0,2070,PENDING
36926,2014-03-10 00:00:00.0,1164,COMPLETE
36927,2014-03-10 00:00:00.0,9197,COMPLETE
36928,2014-03-10 00:00:00.0,7715,PENDING_PAYMENT
36929,2014-03-10 00:00:00.0,6028,PENDING_PAYMENT
36930,2014-03-10 00:00:00.0,3167,ON_HOLD
36931,2014-03-10 00:00:00.0,12140,PENDING_PAYMENT
36932,2014-03-10 00:00:00.0,4470,PENDING_PAYMENT
36933,2014-03-10 00:00:00.0,7246,CLOSED
36934,2014-03-10 00:00:00.0,4771,PENDING_PAYMENT
36935,2014-03-10 00:00:00.0,2220,COMPLETE
36936,2014-03-10 00:00:00.0,4214,PENDING_PAYMENT
36937,2014-03-10 00:00:00.0,8423,PROCESSING
36938,2014-03-10 00:00:00.0,11469,PENDING_PAYMENT
36939,2014-03-10 00:00:00.0,2863,COMPLETE
36940,2014-03-10 00:00:00.0,3626,PENDING_PAYMENT
36941,2014-03-10 00:00:00.0,4780,CLOSED
36942,2014-03-10 00:00:00.0,4661,PENDING_PAYMENT
[cloudera@quickstart SQOOP]$ hadoop fs -tail /user/cloudera/orders/part-m-00002
0,2014-06-23 00:00:00.0,1440,PENDING_PAYMENT
52891,2014-06-23 00:00:00.0,6802,PENDING
52892,2014-06-23 00:00:00.0,9756,PENDING
52893,2014-06-23 00:00:00.0,256,COMPLETE
52894,2014-06-23 00:00:00.0,7234,ON_HOLD
52895,2014-06-23 00:00:00.0,1771,PROCESSING
52896,2014-06-23 00:00:00.0,10293,CLOSED
52897,2014-06-23 00:00:00.0,2464,PENDING_PAYMENT
52898,2014-06-23 00:00:00.0,9771,COMPLETE
52899,2014-06-23 00:00:00.0,1721,PAYMENT_REVIEW
52900,2014-06-23 00:00:00.0,3382,PENDING_PAYMENT
52901,2014-06-23 00:00:00.0,3516,PENDING_PAYMENT
52902,2014-06-23 00:00:00.0,10157,PENDING_PAYMENT
52903,2014-06-23 00:00:00.0,42,PENDING_PAYMENT
52904,2014-06-23 00:00:00.0,1323,COMPLETE
52905,2014-06-23 00:00:00.0,2581,PENDING_PAYMENT
52906,2014-06-23 00:00:00.0,5051,PENDING_PAYMENT
52907,2014-06-23 00:00:00.0,11604,COMPLETE
52908,2014-06-23 00:00:00.0,10449,PENDING_PAYMENT
52909,2014-06-23 00:00:00.0,2941,ON_HOLD
52910,2014-06-23 00:00:00.0,1638,CLOSED
52911,2014-06-23 00:00:00.0,5657,PENDING
52912,2014-06-23 00:00:00.0,6351,PENDING
[cloudera@quickstart SQOOP]$ hadoop fs -tail /user/cloudera/orders/part-m-00003
014-06-12 00:00:00.0,4229,PENDING
68861,2014-06-13 00:00:00.0,3031,PENDING_PAYMENT
68862,2014-06-15 00:00:00.0,7326,PROCESSING
68863,2014-06-16 00:00:00.0,3361,CLOSED
68864,2014-06-18 00:00:00.0,9634,ON_HOLD
68865,2014-06-19 00:00:00.0,4567,SUSPECTED_FRAUD
68866,2014-06-20 00:00:00.0,3890,PENDING_PAYMENT
68867,2014-06-23 00:00:00.0,869,CANCELED
68868,2014-06-24 00:00:00.0,10184,PENDING
68869,2014-06-25 00:00:00.0,7456,PROCESSING
68870,2014-06-26 00:00:00.0,3343,COMPLETE
68871,2014-06-28 00:00:00.0,4960,PENDING
68872,2014-06-29 00:00:00.0,3354,COMPLETE
68873,2014-06-30 00:00:00.0,4545,PENDING
68874,2014-07-03 00:00:00.0,1601,COMPLETE
68875,2014-07-04 00:00:00.0,10637,ON_HOLD
68876,2014-07-06 00:00:00.0,4124,COMPLETE
68877,2014-07-07 00:00:00.0,9692,ON_HOLD
68878,2014-07-08 00:00:00.0,6753,COMPLETE
68879,2014-07-09 00:00:00.0,778,COMPLETE
68880,2014-07-13 00:00:00.0,1117,COMPLETE
68881,2014-07-19 00:00:00.0,2518,PENDING_PAYMENT
68882,2014-07-22 00:00:00.0,10000,ON_HOLD
68883,2014-07-23 00:00:00.0,5533,COMPLETE
[cloudera@quickstart SQOOP]$ 


7. Por ultimo copiamos los archivos de salida a local:

[cloudera@quickstart ~]$ hadoop fs -copyToLocal categories orders /tmp/sqoop/customers /home/cloudera/Desktop/Curso/Ejercicios_Fernando/SQOOP/HDFS
[cloudera@quickstart ~]$ 








